{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyaml in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (21.10.1)\n",
      "Requirement already satisfied: PyYAML in c:\\programdata\\anaconda3\\lib\\site-packages (from pyaml) (6.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dask in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (2023.3.2)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from dask) (2023.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask) (22.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from dask) (6.1.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from dask) (8.1.3)\n",
      "Requirement already satisfied: toolz>=0.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from dask) (0.12.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from dask) (1.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from dask) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click>=7.0->dask) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.13.0->dask) (3.11.0)\n",
      "Requirement already satisfied: locket in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from partd>=1.2.0->dask) (1.0.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: modin in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (0.19.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from modin) (1.24.2)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from modin) (5.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from modin) (2023.3.0)\n",
      "Requirement already satisfied: pandas==1.5.3 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from modin) (1.5.3)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from modin) (22.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from pandas==1.5.3->modin) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from pandas==1.5.3->modin) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.1->pandas==1.5.3->modin) (1.15.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ray in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.32.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ray) (1.42.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from ray) (8.1.3)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from ray) (3.19.6)\n",
      "Requirement already satisfied: aiosignal in c:\\programdata\\anaconda3\\lib\\site-packages (from ray) (1.2.0)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from ray) (6.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from ray) (1.0.5)\n",
      "Requirement already satisfied: attrs in c:\\programdata\\anaconda3\\lib\\site-packages (from ray) (22.1.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from ray) (1.24.2)\n",
      "Requirement already satisfied: jsonschema in c:\\programdata\\anaconda3\\lib\\site-packages (from ray) (4.16.0)\n",
      "Requirement already satisfied: virtualenv>=20.0.24 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from ray) (20.21.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from ray) (3.10.7)\n",
      "Requirement already satisfied: frozenlist in c:\\programdata\\anaconda3\\lib\\site-packages (from ray) (1.3.3)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from ray) (2.28.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click>=7.0->ray) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from grpcio>=1.32.0->ray) (1.15.0)\n",
      "Requirement already satisfied: platformdirs<4,>=2.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from virtualenv>=20.0.24->ray) (2.5.2)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in c:\\users\\yashd\\appdata\\roaming\\python\\python39\\site-packages (from virtualenv>=20.0.24->ray) (0.3.6)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema->ray) (0.18.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->ray) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->ray) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->ray) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->ray) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "#Installing the required libraries\n",
    "!pip install pyaml\n",
    "!pip install dask\n",
    "!pip install modin\n",
    "!pip install ray"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing the required libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ray\n",
    "import modin.pandas as mpd\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "import timeit\n",
    "import csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making a sample large csv file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated file size: 2748.542417526245 MB\n"
     ]
    }
   ],
   "source": [
    "num_rows = 30_000_000\n",
    "header = ['id', 'name', 'age', 'city', 'country'] # Column names\n",
    "\n",
    "data = []\n",
    "for i in range(num_rows):\n",
    "    row = [f'ID{i+1}', f'first_name{i+1} middle_name{i+1} last_name{i+1}', 20+i%10, f'city{i%5+1}, state{i%20+1}', 'country1']\n",
    "    data.append(row)\n",
    "\n",
    "file_path = 'large_csv.csv'\n",
    "with open(file_path, 'w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(data)\n",
    "\n",
    "# Check file size\n",
    "file_size = os.path.getsize(file_path)\n",
    "print(f\"Generated file size: {file_size/1024/1024} MB\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking time taken to read the csv file through different methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "file_path = \"large_csv.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 133.68586099999993 seconds\n",
      "Dask: 0.14022990000012214 seconds\n",
      "Modin: 137.4125736000001 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pandas: {timeit.timeit(lambda: pd.read_csv(file_path), number=1)} seconds\")\n",
    "print(f\"Dask: {timeit.timeit(lambda: dd.read_csv(file_path), number=1)} seconds\")\n",
    "print(f\"Modin: {timeit.timeit(lambda: mpd.read_csv(file_path), number=1)} seconds\")\n",
    "ray.shutdown() #Shutting down modin ray instance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 21:29:37,891\tERROR services.py:1169 -- Failed to start the dashboard \n",
      "2023-04-01 21:29:37,891\tERROR services.py:1194 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.\n",
      "2023-04-01 21:29:37,891\tERROR services.py:1204 -- Couldn't read dashboard.log file. Error: [Errno 2] No such file or directory: 'C:\\\\Users\\\\yashd\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2023-04-01_21-29-13_887448_3648\\\\logs\\\\dashboard.log'. It means the dashboard is broken even before it initializes the logger (mostly dependency issues). Reading the dashboard.err file which contains stdout/stderr.\n",
      "2023-04-01 21:29:37,891\tERROR services.py:1238 -- Failed to read dashboard.err file: cannot mmap an empty file. It is unexpected. Please report an issue to Ray github. https://github.com/ray-project/ray/issues\n",
      "2023-04-01 21:29:44,527\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "@ray.remote\n",
    "def ray_read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "ray.init()\n",
    "print(f\"Ray: {timeit.timeit(lambda: ray.get(ray_read_csv.remote(file_path)), number=1)} seconds\")\n",
    "ray.shutdown()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Dask seems to be the most efficient for our case"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Writing yaml and utility file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting file_metadata.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile file_metadata.yaml\n",
    "file_type: csv\n",
    "dataset_name: person_details\n",
    "file_name: large_csv\n",
    "inbound_delimiter: \",\"\n",
    "outbound_delimiter: \"|\"\n",
    "columns:\n",
    "    - ID\n",
    "    - Name\n",
    "    - Age\n",
    "    - City\n",
    "    - Country"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utility.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utility.py\n",
    "import yaml\n",
    "import re\n",
    "def read_config_file(path):\n",
    "    with open(path,'r') as file:\n",
    "        try:\n",
    "            return yaml.safe_load(stream=file)\n",
    "        except Exception as e:\n",
    "            return e\n",
    "\n",
    "def header_preprocessor(header_list):\n",
    "    header_preprocessed_list=[]\n",
    "    for header in header_list:\n",
    "        header_preprocessed = header.lower() # Converting header to lower case\n",
    "        header_preprocessed = header_preprocessed.strip() # Removing extra spaces\n",
    "        header_preprocessed = re.sub(\"[\\s_@]+\",\"_\",header_preprocessed) # Substituting special characters\n",
    "        header_preprocessed_list.append(header_preprocessed) # Appending preprocessed header to list\n",
    "    return header_preprocessed_list\n",
    "\n",
    "def column_header_validation(df_columns,config_columns):\n",
    "    df_cols_preprocessed = header_preprocessor(df_columns)\n",
    "    config_cols_preprocessed = header_preprocessor(config_columns)\n",
    "\n",
    "    if df_cols_preprocessed==config_cols_preprocessed:\n",
    "        return \"Column validation passed\"\n",
    "    elif len(df_cols_preprocessed)==len(config_cols_preprocessed):\n",
    "        print(\"Length of columns is same but column names are different\")\n",
    "        print(f\"Following column headers not present in config file: {[x for x in df_cols_preprocessed if x not in config_cols_preprocessed]}\")\n",
    "        print(f\"Following columns of config file not found in dataframe: {[x for x in config_cols_preprocessed if x not in df_cols_preprocessed]}\")\n",
    "    else:\n",
    "        print(\"Entire column validation failed (both names and length are different)\")\n",
    "        print(f\"Following columns of dataframe not found in config file: {[x for x in df_cols_preprocessed if x not in config_cols_preprocessed]}\")\n",
    "        print(f\"Following columns of config file not found in dataframe: {[x for x in config_cols_preprocessed if x not in df_cols_preprocessed]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing sample data and validating it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#Defining config file path\n",
    "config_file_path = 'file_metadata.yaml'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import utility\n",
    "#Reading configuration file\n",
    "config_file = utility.read_config_file(config_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{'file_type': 'csv',\n 'dataset_name': 'person_details',\n 'file_name': 'large_csv',\n 'inbound_delimiter': ',',\n 'outbound_delimiter': '|',\n 'columns': ['ID', 'Name', 'Age', 'City', 'Country']}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Defining data file path\n",
    "data_path = \"D:/DataspellProjects/Data Science/Data Glacier Internship/\"+config_file['file_name']+f\".{config_file['file_type']}\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Reading the csv file using dask\n",
    "df = dd.read_csv(data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "'Column validation passed'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validating the imported dataframe with config data\n",
    "utility.column_header_validation(list(df.columns.values),list(config_file['columns']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Printing Data Ingestion Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Ingestion Summary\n",
      "Total number of rows: 30000000\n",
      "Total number of columns: 5\n",
      "File size: 2.6841234546154737 GB\n",
      "File type: csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Ingestion Summary\")\n",
    "print(f\"Total number of rows: {df.shape[0].compute()}\")\n",
    "print(f\"Total number of columns: {df.shape[1]}\")\n",
    "print(f\"File size: {os.path.getsize(data_path)/(1024*1024*1024)} GB\")\n",
    "print(f\"File type: {config_file['file_type']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving the dataframe in pipe separated text file (|) in gz format"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "['D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\00.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\01.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\02.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\03.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\04.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\05.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\06.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\07.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\08.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\09.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\10.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\11.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\12.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\13.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\14.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\15.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\16.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\17.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\18.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\19.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\20.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\21.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\22.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\23.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\24.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\25.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\26.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\27.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\28.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\29.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\30.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\31.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\32.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\33.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\34.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\35.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\36.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\37.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\38.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\39.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\40.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\41.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\42.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\43.part',\n 'D:\\\\DataspellProjects\\\\Data Science\\\\Data Glacier Internship\\\\Data-Glacier-Intern-Projects-Week-6\\\\large_gz.gz\\\\44.part']"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('large_gz.gz',sep=config_file['outbound_delimiter'],header=True,index=False,compression='gzip')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total .gz file count: 45\n",
      ".gz folder size: 8.0 KB\n"
     ]
    }
   ],
   "source": [
    "#Getting number of files and total folder size\n",
    "file_count=0\n",
    "for file in os.listdir('large_gz.gz'):\n",
    "    file_count+=1\n",
    "print(f\"Total .gz file count: {file_count}\")\n",
    "print(f\".gz folder size: {os.path.getsize('large_gz.gz')/(1024)} KB\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experimenting with column validation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_copy = df_copy.rename(columns={'name':' name '})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "'Column validation passed'"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility.column_header_validation(df_copy.columns.values,config_file['columns'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "df_copy_columns = list(df_copy.columns.values)\n",
    "df_copy_columns.append('test col 1')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire column validation failed (both names and length are different)\n",
      "Following columns of dataframe not found in config file: ['test_col_1']\n",
      "Following columns of config file not found in dataframe: []\n"
     ]
    }
   ],
   "source": [
    "utility.column_header_validation(df_copy_columns,config_file['columns'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of columns is same but column names are different\n",
      "Following column headers not present in config file: ['_name_']\n",
      "Following columns of config file not found in dataframe: ['name']\n"
     ]
    }
   ],
   "source": [
    "df_copy = df_copy.rename(columns={' name ':'__name__ '})\n",
    "utility.column_header_validation(df_copy.columns.values,config_file['columns'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}